{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"anlearn - Anomaly learn anlearn is a python package that aims to implement multiple state-of-the-art anomaly detection methods in familiar scikit-learn API. Installation anlearn depends on scikit-learn and it's dependencies scipy and numpy . Requirements: python >=3.6 scikit-learn scipy numpy Requirements for every supported python version with version and hashes could be found in requirements folder. We're using pip-tools for generating requirements files. Intallation options PyPI installation pip install anlearn Installation from source git clone https://github.com/gaussalgo/anlearn cd anlearn Installing requirements. # Generated requirements for all supported python versions ls requirements/requirements-3.*.txt | grep -v dev requirements/requirements-3.6.txt requirements/requirements-3.7.txt requirements/requirements-3.8.txt pip install -r requirements/requirements-3.8.txt or pip install scikit-learn numpy scipy Install anlearn . pip install . or python setup.py install License GNU Lesser General Public License v3 or later (LGPLv3+) anlearn Copyright (C) 2020 Gauss Algorithmic a.s. This package is in alpha staten and comes with ABSOLUTELY NO WARRANTY. This is free software, and you are welcome to use, redistribute it and contribute under certain conditions of it's license.","title":"Home"},{"location":"#anlearn-anomaly-learn","text":"anlearn is a python package that aims to implement multiple state-of-the-art anomaly detection methods in familiar scikit-learn API.","title":"anlearn - Anomaly learn"},{"location":"#installation","text":"anlearn depends on scikit-learn and it's dependencies scipy and numpy . Requirements: python >=3.6 scikit-learn scipy numpy Requirements for every supported python version with version and hashes could be found in requirements folder. We're using pip-tools for generating requirements files.","title":"Installation"},{"location":"#intallation-options","text":"","title":"Intallation options"},{"location":"#pypi-installation","text":"pip install anlearn","title":"PyPI installation"},{"location":"#installation-from-source","text":"git clone https://github.com/gaussalgo/anlearn cd anlearn Installing requirements. # Generated requirements for all supported python versions ls requirements/requirements-3.*.txt | grep -v dev requirements/requirements-3.6.txt requirements/requirements-3.7.txt requirements/requirements-3.8.txt pip install -r requirements/requirements-3.8.txt or pip install scikit-learn numpy scipy Install anlearn . pip install . or python setup.py install","title":"Installation from source"},{"location":"#license","text":"GNU Lesser General Public License v3 or later (LGPLv3+) anlearn Copyright (C) 2020 Gauss Algorithmic a.s. This package is in alpha staten and comes with ABSOLUTELY NO WARRANTY. This is free software, and you are welcome to use, redistribute it and contribute under certain conditions of it's license.","title":"License"},{"location":"dev-guide/","text":"Developers Guide Tools As anlearn developers, we're using these tools. Code formating: black isort Linting: flake8 mypy Requirements: pip-tools Testing: pytest Documentation: MkDocs Material for MkDocs Other: pre-commit nix-shell direnv Setting-up the developers' environment Nix-shell & direnv For easy environment management, we're using nix-shell in combination with direnv . Using these two tools reduces the time and effort required to create and maintain a deterministic environment. We highly recommend using them. Nix configuration is in shell.nix and for direnv in .envrc . Python tools As for python versions currently, support is for python 3.6, 3.7, and 3.8. To ensure a similar code style choice for formating is black and isort . As a linters we use mypy and flake8 . For easier code check before committing any changes, there is an option to use the pre-commit tool. As you can see in .pre-commit-config.yaml it is using only currently installed versions of black, isort, and flake8. Installation It's highly recommended to use python virtual environment for development. All tools with their specified version are in the requirements-dev files. pip install -r requirements/requirements-3.8-dev.txt For pre-commit pre-commit install (after installing tools) Configuration files Configuration for isort , pytest , flake8 , and mypy is in setup.cfg file. Configuration for pre-commit is in .pre-commit-config.yaml file. Generating requirements - pip-tools For easier testing, we're using pinned requirements for every supported python version. For this purpose, there is the pip-tools package. Configuration for generating requirement files is in Makefile. The easiest way to generate them is by using a pre-prepared make file. make requirements If you want only to generate requirements for one specific python version you could use make requirements-3.8 Tests All tests are in test folder. We're using pytest for testing. Documentation We're using MkDocs in combination with Material for MkDocs . For generating documentation, you need to have requirements-dev and anlearn installed. Then you can simply run mkdocs serve or make docs .","title":"Developers Guide"},{"location":"dev-guide/#developers-guide","text":"","title":"Developers Guide"},{"location":"dev-guide/#tools","text":"As anlearn developers, we're using these tools. Code formating: black isort Linting: flake8 mypy Requirements: pip-tools Testing: pytest Documentation: MkDocs Material for MkDocs Other: pre-commit nix-shell direnv","title":"Tools"},{"location":"dev-guide/#setting-up-the-developers-environment","text":"","title":"Setting-up the developers' environment"},{"location":"dev-guide/#nix-shell-direnv","text":"For easy environment management, we're using nix-shell in combination with direnv . Using these two tools reduces the time and effort required to create and maintain a deterministic environment. We highly recommend using them. Nix configuration is in shell.nix and for direnv in .envrc .","title":"Nix-shell &amp; direnv"},{"location":"dev-guide/#python-tools","text":"As for python versions currently, support is for python 3.6, 3.7, and 3.8. To ensure a similar code style choice for formating is black and isort . As a linters we use mypy and flake8 . For easier code check before committing any changes, there is an option to use the pre-commit tool. As you can see in .pre-commit-config.yaml it is using only currently installed versions of black, isort, and flake8.","title":"Python tools"},{"location":"dev-guide/#installation","text":"It's highly recommended to use python virtual environment for development. All tools with their specified version are in the requirements-dev files. pip install -r requirements/requirements-3.8-dev.txt For pre-commit pre-commit install (after installing tools)","title":"Installation"},{"location":"dev-guide/#configuration-files","text":"Configuration for isort , pytest , flake8 , and mypy is in setup.cfg file. Configuration for pre-commit is in .pre-commit-config.yaml file.","title":"Configuration files"},{"location":"dev-guide/#generating-requirements-pip-tools","text":"For easier testing, we're using pinned requirements for every supported python version. For this purpose, there is the pip-tools package. Configuration for generating requirement files is in Makefile. The easiest way to generate them is by using a pre-prepared make file. make requirements If you want only to generate requirements for one specific python version you could use make requirements-3.8","title":"Generating requirements - pip-tools"},{"location":"dev-guide/#tests","text":"All tests are in test folder. We're using pytest for testing.","title":"Tests"},{"location":"dev-guide/#documentation","text":"We're using MkDocs in combination with Material for MkDocs . For generating documentation, you need to have requirements-dev and anlearn installed. Then you can simply run mkdocs serve or make docs .","title":"Documentation"},{"location":"jupyter-notebooks/","text":"Jupyter notebooks We're trying to provide multiple useful and interesting examples in the form of Jupyter notebooks. You can find them in the notebooks folder. In these examples, we'll try to show how to use, the advantages and disadvantages of anomaly detection methods found in anlearn library. Also, we'll provide some background for them. Installation Running notebooks with kernel from virtual environment (recommended): Activate virtual environment. source venv/bin/activate Install requirements. We're providing requirements-notebook.txt with version and hashes for Python 3.8. If you're using other python or you don't want to use the same versions as we did, you could use requirements-notebook.in . Python 3.8 pip install -r requirements/requirements-notebook.txt other Python versions (or if you don't want to use specific version of libraries) pip install -r requirements/requirements-notebook.in Install ipython kernel python -m ipykernel install --user --name anlearn-env --display-name \"Anomaly learn\" Now you can run jupyter notebooks with kernel from your virtual environment. jupyter notebook","title":"Jupyter notebooks"},{"location":"jupyter-notebooks/#jupyter-notebooks","text":"We're trying to provide multiple useful and interesting examples in the form of Jupyter notebooks. You can find them in the notebooks folder. In these examples, we'll try to show how to use, the advantages and disadvantages of anomaly detection methods found in anlearn library. Also, we'll provide some background for them.","title":"Jupyter notebooks"},{"location":"jupyter-notebooks/#installation","text":"Running notebooks with kernel from virtual environment (recommended): Activate virtual environment. source venv/bin/activate Install requirements. We're providing requirements-notebook.txt with version and hashes for Python 3.8. If you're using other python or you don't want to use the same versions as we did, you could use requirements-notebook.in . Python 3.8 pip install -r requirements/requirements-notebook.txt other Python versions (or if you don't want to use specific version of libraries) pip install -r requirements/requirements-notebook.in Install ipython kernel python -m ipykernel install --user --name anlearn-env --display-name \"Anomaly learn\" Now you can run jupyter notebooks with kernel from your virtual environment. jupyter notebook","title":"Installation"},{"location":"loda/","text":"Loda: Lightweight on-line detector of anomalies Source code import time import numpy as np import matplotlib import matplotlib.pyplot as plt from sklearn import svm from sklearn.datasets import make_moons , make_blobs from sklearn.covariance import EllipticEnvelope from sklearn.ensemble import IsolationForest from sklearn.neighbors import LocalOutlierFactor from anlearn.loda import LODA print ( __doc__ ) matplotlib . rcParams [ 'contour.negative_linestyle' ] = 'solid' # Example settings n_samples = 300 outliers_fraction = 0.15 n_outliers = int ( outliers_fraction * n_samples ) n_inliers = n_samples - n_outliers # define outlier/anomaly detection methods to be compared anomaly_algorithms = [ ( \"Robust covariance\" , EllipticEnvelope ( contamination = outliers_fraction )), ( \"One-Class SVM\" , svm . OneClassSVM ( nu = outliers_fraction , kernel = \"rbf\" , gamma = 0.1 )), ( \"Isolation Forest\" , IsolationForest ( contamination = outliers_fraction , random_state = 42 )), ( \"Local Outlier Factor\" , LocalOutlierFactor ( n_neighbors = 35 , contamination = outliers_fraction )), ( \"LODA\" , LODA ( n_estimators = 100 , q = outliers_fraction , random_state = 42 )) ] # Define datasets blobs_params = dict ( random_state = 0 , n_samples = n_inliers , n_features = 2 ) datasets = [ make_blobs ( centers = [[ 0 , 0 ], [ 0 , 0 ]], cluster_std = 0.5 , ** blobs_params )[ 0 ], make_blobs ( centers = [[ 2 , 2 ], [ - 2 , - 2 ]], cluster_std = [ 0.5 , 0.5 ], ** blobs_params )[ 0 ], make_blobs ( centers = [[ 2 , 2 ], [ - 2 , - 2 ]], cluster_std = [ 1.5 , . 3 ], ** blobs_params )[ 0 ], 1. * ( make_moons ( n_samples = n_samples , noise =. 05 , random_state = 0 )[ 0 ] - np . array ([ 0.5 , 0.25 ])), 2. * ( np . random . RandomState ( 42 ) . rand ( n_samples , 2 ) - 0.5 )] # Compare given classifiers under given settings xx , yy = np . meshgrid ( np . linspace ( - 7 , 7 , 150 ), np . linspace ( - 7 , 7 , 150 )) plt . figure ( figsize = ( len ( anomaly_algorithms ) * 2 + 3 , 12.5 )) plt . subplots_adjust ( left =. 02 , right =. 98 , bottom =. 001 , top =. 96 , wspace =. 05 , hspace =. 01 ) plot_num = 1 rng = np . random . RandomState ( 42 ) for i_dataset , X in enumerate ( datasets ): # Add outliers X = np . concatenate ([ X , rng . uniform ( low =- 6 , high = 6 , size = ( n_outliers , 2 ))], axis = 0 ) for name , algorithm in anomaly_algorithms : t0 = time . time () algorithm . fit ( X ) t1 = time . time () plt . subplot ( len ( datasets ), len ( anomaly_algorithms ), plot_num ) if i_dataset == 0 : plt . title ( name , size = 18 ) # fit the data and tag outliers if name == \"Local Outlier Factor\" : y_pred = algorithm . fit_predict ( X ) else : y_pred = algorithm . fit ( X ) . predict ( X ) # plot the levels lines and the points if name != \"Local Outlier Factor\" : # LOF does not implement predict Z = algorithm . predict ( np . c_ [ xx . ravel (), yy . ravel ()]) Z = Z . reshape ( xx . shape ) plt . contour ( xx , yy , Z , levels = [ 0 ], linewidths = 1.5 , colors = 'black' ) colors = np . array ([ '#377eb8' , '#ff7f00' ]) plt . scatter ( X [:, 0 ], X [:, 1 ], s = 10 , color = colors [( y_pred + 1 ) // 2 ]) plt . xlim ( - 7 , 7 ) plt . ylim ( - 7 , 7 ) plt . xticks (()) plt . yticks (()) plt . text ( . 99 , . 01 , ( ' %.2f s' % ( t1 - t0 )) . lstrip ( '0' ), transform = plt . gca () . transAxes , size = 15 , horizontalalignment = 'right' ) plot_num += 1 plt . show ()","title":"Loda: Lightweight on-line detector of anomalies"},{"location":"loda/#loda-lightweight-on-line-detector-of-anomalies","text":"Source code import time import numpy as np import matplotlib import matplotlib.pyplot as plt from sklearn import svm from sklearn.datasets import make_moons , make_blobs from sklearn.covariance import EllipticEnvelope from sklearn.ensemble import IsolationForest from sklearn.neighbors import LocalOutlierFactor from anlearn.loda import LODA print ( __doc__ ) matplotlib . rcParams [ 'contour.negative_linestyle' ] = 'solid' # Example settings n_samples = 300 outliers_fraction = 0.15 n_outliers = int ( outliers_fraction * n_samples ) n_inliers = n_samples - n_outliers # define outlier/anomaly detection methods to be compared anomaly_algorithms = [ ( \"Robust covariance\" , EllipticEnvelope ( contamination = outliers_fraction )), ( \"One-Class SVM\" , svm . OneClassSVM ( nu = outliers_fraction , kernel = \"rbf\" , gamma = 0.1 )), ( \"Isolation Forest\" , IsolationForest ( contamination = outliers_fraction , random_state = 42 )), ( \"Local Outlier Factor\" , LocalOutlierFactor ( n_neighbors = 35 , contamination = outliers_fraction )), ( \"LODA\" , LODA ( n_estimators = 100 , q = outliers_fraction , random_state = 42 )) ] # Define datasets blobs_params = dict ( random_state = 0 , n_samples = n_inliers , n_features = 2 ) datasets = [ make_blobs ( centers = [[ 0 , 0 ], [ 0 , 0 ]], cluster_std = 0.5 , ** blobs_params )[ 0 ], make_blobs ( centers = [[ 2 , 2 ], [ - 2 , - 2 ]], cluster_std = [ 0.5 , 0.5 ], ** blobs_params )[ 0 ], make_blobs ( centers = [[ 2 , 2 ], [ - 2 , - 2 ]], cluster_std = [ 1.5 , . 3 ], ** blobs_params )[ 0 ], 1. * ( make_moons ( n_samples = n_samples , noise =. 05 , random_state = 0 )[ 0 ] - np . array ([ 0.5 , 0.25 ])), 2. * ( np . random . RandomState ( 42 ) . rand ( n_samples , 2 ) - 0.5 )] # Compare given classifiers under given settings xx , yy = np . meshgrid ( np . linspace ( - 7 , 7 , 150 ), np . linspace ( - 7 , 7 , 150 )) plt . figure ( figsize = ( len ( anomaly_algorithms ) * 2 + 3 , 12.5 )) plt . subplots_adjust ( left =. 02 , right =. 98 , bottom =. 001 , top =. 96 , wspace =. 05 , hspace =. 01 ) plot_num = 1 rng = np . random . RandomState ( 42 ) for i_dataset , X in enumerate ( datasets ): # Add outliers X = np . concatenate ([ X , rng . uniform ( low =- 6 , high = 6 , size = ( n_outliers , 2 ))], axis = 0 ) for name , algorithm in anomaly_algorithms : t0 = time . time () algorithm . fit ( X ) t1 = time . time () plt . subplot ( len ( datasets ), len ( anomaly_algorithms ), plot_num ) if i_dataset == 0 : plt . title ( name , size = 18 ) # fit the data and tag outliers if name == \"Local Outlier Factor\" : y_pred = algorithm . fit_predict ( X ) else : y_pred = algorithm . fit ( X ) . predict ( X ) # plot the levels lines and the points if name != \"Local Outlier Factor\" : # LOF does not implement predict Z = algorithm . predict ( np . c_ [ xx . ravel (), yy . ravel ()]) Z = Z . reshape ( xx . shape ) plt . contour ( xx , yy , Z , levels = [ 0 ], linewidths = 1.5 , colors = 'black' ) colors = np . array ([ '#377eb8' , '#ff7f00' ]) plt . scatter ( X [:, 0 ], X [:, 1 ], s = 10 , color = colors [( y_pred + 1 ) // 2 ]) plt . xlim ( - 7 , 7 ) plt . ylim ( - 7 , 7 ) plt . xticks (()) plt . yticks (()) plt . text ( . 99 , . 01 , ( ' %.2f s' % ( t1 - t0 )) . lstrip ( '0' ), transform = plt . gca () . transAxes , size = 15 , horizontalalignment = 'right' ) plot_num += 1 plt . show ()","title":"Loda: Lightweight on-line detector of anomalies"},{"location":"reference/loda/","text":"LODA LODA: Lightweight on-line detector of anomalies __init__ ( self , n_estimators = 1000 , bins = 'auto' , q = 0.05 , random_state = None , n_jobs = None , verbose = 0 ) special LODA: Lightweight on-line detector of anomalies [1] LODA is an ensemble of histograms on random projections. See Pevn\u00fd, T. Loda [1] for more details. Parameters: Name Type Description Default n_estimators int number of histograms. Defaults to 1000. 1000 bins Union[int, str] int - number of equal-width bins in the given range. str - method used to calculate bin width ( numpy.histogram_bin_edges ). See numpy.histogram bins for more details. Defaults to \"auto\". 'auto' q float Quantile for compution threshold from training data scores. This threshold is used for predict method. Defaults to 0.05. 0.05 random_state Optional[int] Random seed used for stochastic parts. Defaults to None. None n_jobs Optional[int] Not implemented yet. Defaults to None. None verbose int Verbosity of logging. Defaults to 0. 0 Attributes projections_ (numpy.ndarray, shape (n_estimators, n_features)) : Random projections hists_ (List[scipy.stats.rv_histogram], shape (n_estimators,)) : Histograms anom_threshold_ (float) : Treshold for predict function. References Pevn\u00fd, T. Loda: Lightweight on-line detector of anomalies. Mach Learn 102, 275\u2013304 (2016). https://doi.org/10.1007/s10994-015-5521-0 Source code in anlearn/loda.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def __init__ ( self , n_estimators : int = 1000 , bins : Union [ int , str ] = \"auto\" , q : float = 0.05 , random_state : Optional [ int ] = None , n_jobs : Optional [ int ] = None , verbose : int = 0 , ) -> None : \"\"\"LODA: Lightweight on-line detector of anomalies [1] LODA is an ensemble of histograms on random projections. See Pevn\u00fd, T. Loda [1] for more details. Arguments: n_estimators (int, optional): number of histograms. Defaults to 1000. bins (Union[int, str], optional): * `int` - number of equal-width bins in the given range. * `str` - method used to calculate bin width (`numpy.histogram_bin_edges`). See `numpy.histogram` bins for more details. Defaults to \"auto\". q (float, optional): Quantile for compution threshold from training data scores. This threshold is used for ``predict`` method. Defaults to 0.05. random_state (Optional[int], optional): Random seed used for stochastic parts. Defaults to None. n_jobs (Optional[int], optional): Not implemented yet. Defaults to None. verbose (int, optional): Verbosity of logging. Defaults to 0. Attributes: * projections_ (numpy.ndarray, shape (n_estimators, n_features)) : Random projections * hists_ (List[scipy.stats.rv_histogram], shape (n_estimators,)) : Histograms * anom_threshold_ (float) : Treshold for `predict` function. References: 1. Pevn\u00fd, T. Loda: Lightweight on-line detector of anomalies. Mach Learn 102, 275\u2013304 (2016). <https://doi.org/10.1007/s10994-015-5521-0> \"\"\" self . n_estimators = n_estimators self . bins = bins self . q = q self . random_state = random_state self . verbose = verbose self . n_jobs = n_jobs # TODO self . _validate () fit ( self , X , y = None ) Fit estimator Parameters: Name Type Description Default X ~ArrayLike shape (n_samples, n_features). Input data required y Optional[~ArrayLike] ignored. Present for API consistency by convention. Defaults to None. None Returns: Type Description LODA LODA: [description] Source code in anlearn/loda.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def fit ( self , X : ArrayLike , y : Optional [ ArrayLike ] = None ) -> \"LODA\" : \"\"\"Fit estimator Args: X (ArrayLike): shape (n_samples, n_features). Input data y (Optional[ArrayLike], optional): ignored. Present for API consistency by convention. Defaults to None. Returns: LODA: [description] \"\"\" raw_data = check_array ( X , accept_sparse = False , dtype = \"numeric\" , force_all_finite = True ) self . _shape = raw_data . shape self . _init_projections () w_X = X @ self . projections_ . T self . hists_ = [] X_prob = [] for w_x in w_X . T : new_hist = HistPdf ( bins = self . bins , return_min = True ) . fit ( w_x ) self . hists_ . append ( new_hist ) prob = new_hist . predict_proba ( w_x ) X_prob . append ( prob ) X_scores = np . mean ( np . log ( X_prob ), axis = 0 ) self . anom_threshold_ = np . quantile ( X_scores , self . q ) return self predict ( self , X ) Predict if samples are outliers or not Samples with a score lower than anom_threshold_ are considered to be outliers. Parameters: Name Type Description Default X ~ArrayLike hape (n_samples, n_features). Input data required Returns: Type Description ndarray np.ndarray: shape (n_samples,) 1 for inlineres, -1 for outliers Source code in anlearn/loda.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def predict ( self , X : ArrayLike ) -> np . ndarray : \"\"\"Predict if samples are outliers or not Samples with a score lower than ``anom_threshold_`` are considered to be outliers. Args: X (ArrayLike): hape (n_samples, n_features). Input data Returns: np.ndarray: shape (n_samples,) 1 for inlineres, -1 for outliers \"\"\" check_is_fitted ( self , attributes = [ \"anom_threshold_\" ]) scores = self . score_samples ( X ) return np . where ( scores < self . anom_threshold_ , - 1 , 1 ) score_samples ( self , X ) Anomaly scores for samples Average of the logarithm probabilities estimated of individual projections. Output is proportional to the negative log-likelihood of the sample, that means the less likely a sample is, the higher the anomaly value it receives[1]_. This score is reversed for Scikit learn compatibility. Parameters: Name Type Description Default X ~ArrayLike shape (n_samples, n_features). Input data required Returns: Type Description ndarray np.ndarray: shape (n_samples,) The anomaly score of the input samples. The lower, the more abnormal. Source code in anlearn/loda.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def score_samples ( self , X : ArrayLike ) -> np . ndarray : \"\"\"Anomaly scores for samples Average of the logarithm probabilities estimated of individual projections. Output is proportional to the negative log-likelihood of the sample, that means the less likely a sample is, the higher the anomaly value it receives[1]_. This score is reversed for Scikit learn compatibility. Args: X (ArrayLike): shape (n_samples, n_features). Input data Returns: np.ndarray: shape (n_samples,) The anomaly score of the input samples. The lower, the more abnormal. \"\"\" check_is_fitted ( self , attributes = [ \"projections_\" , \"hists_\" ]) w_X = X @ self . projections_ . T X_prob = [ hist . predict_proba ( w_x ) for hist , w_x in zip ( self . hists_ , w_X . T )] X_scores = np . mean ( np . log ( X_prob ), axis = 0 ) return X_scores","title":"anlearn.loda"},{"location":"reference/loda/#anlearn.loda","text":"","title":"anlearn.loda"},{"location":"reference/loda/#anlearn.loda.LODA","text":"LODA: Lightweight on-line detector of anomalies","title":"LODA"},{"location":"reference/loda/#anlearn.loda.LODA.__init__","text":"LODA: Lightweight on-line detector of anomalies [1] LODA is an ensemble of histograms on random projections. See Pevn\u00fd, T. Loda [1] for more details. Parameters: Name Type Description Default n_estimators int number of histograms. Defaults to 1000. 1000 bins Union[int, str] int - number of equal-width bins in the given range. str - method used to calculate bin width ( numpy.histogram_bin_edges ). See numpy.histogram bins for more details. Defaults to \"auto\". 'auto' q float Quantile for compution threshold from training data scores. This threshold is used for predict method. Defaults to 0.05. 0.05 random_state Optional[int] Random seed used for stochastic parts. Defaults to None. None n_jobs Optional[int] Not implemented yet. Defaults to None. None verbose int Verbosity of logging. Defaults to 0. 0 Attributes projections_ (numpy.ndarray, shape (n_estimators, n_features)) : Random projections hists_ (List[scipy.stats.rv_histogram], shape (n_estimators,)) : Histograms anom_threshold_ (float) : Treshold for predict function. References Pevn\u00fd, T. Loda: Lightweight on-line detector of anomalies. Mach Learn 102, 275\u2013304 (2016). https://doi.org/10.1007/s10994-015-5521-0 Source code in anlearn/loda.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def __init__ ( self , n_estimators : int = 1000 , bins : Union [ int , str ] = \"auto\" , q : float = 0.05 , random_state : Optional [ int ] = None , n_jobs : Optional [ int ] = None , verbose : int = 0 , ) -> None : \"\"\"LODA: Lightweight on-line detector of anomalies [1] LODA is an ensemble of histograms on random projections. See Pevn\u00fd, T. Loda [1] for more details. Arguments: n_estimators (int, optional): number of histograms. Defaults to 1000. bins (Union[int, str], optional): * `int` - number of equal-width bins in the given range. * `str` - method used to calculate bin width (`numpy.histogram_bin_edges`). See `numpy.histogram` bins for more details. Defaults to \"auto\". q (float, optional): Quantile for compution threshold from training data scores. This threshold is used for ``predict`` method. Defaults to 0.05. random_state (Optional[int], optional): Random seed used for stochastic parts. Defaults to None. n_jobs (Optional[int], optional): Not implemented yet. Defaults to None. verbose (int, optional): Verbosity of logging. Defaults to 0. Attributes: * projections_ (numpy.ndarray, shape (n_estimators, n_features)) : Random projections * hists_ (List[scipy.stats.rv_histogram], shape (n_estimators,)) : Histograms * anom_threshold_ (float) : Treshold for `predict` function. References: 1. Pevn\u00fd, T. Loda: Lightweight on-line detector of anomalies. Mach Learn 102, 275\u2013304 (2016). <https://doi.org/10.1007/s10994-015-5521-0> \"\"\" self . n_estimators = n_estimators self . bins = bins self . q = q self . random_state = random_state self . verbose = verbose self . n_jobs = n_jobs # TODO self . _validate ()","title":"__init__()"},{"location":"reference/loda/#anlearn.loda.LODA.fit","text":"Fit estimator Parameters: Name Type Description Default X ~ArrayLike shape (n_samples, n_features). Input data required y Optional[~ArrayLike] ignored. Present for API consistency by convention. Defaults to None. None Returns: Type Description LODA LODA: [description] Source code in anlearn/loda.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def fit ( self , X : ArrayLike , y : Optional [ ArrayLike ] = None ) -> \"LODA\" : \"\"\"Fit estimator Args: X (ArrayLike): shape (n_samples, n_features). Input data y (Optional[ArrayLike], optional): ignored. Present for API consistency by convention. Defaults to None. Returns: LODA: [description] \"\"\" raw_data = check_array ( X , accept_sparse = False , dtype = \"numeric\" , force_all_finite = True ) self . _shape = raw_data . shape self . _init_projections () w_X = X @ self . projections_ . T self . hists_ = [] X_prob = [] for w_x in w_X . T : new_hist = HistPdf ( bins = self . bins , return_min = True ) . fit ( w_x ) self . hists_ . append ( new_hist ) prob = new_hist . predict_proba ( w_x ) X_prob . append ( prob ) X_scores = np . mean ( np . log ( X_prob ), axis = 0 ) self . anom_threshold_ = np . quantile ( X_scores , self . q ) return self","title":"fit()"},{"location":"reference/loda/#anlearn.loda.LODA.predict","text":"Predict if samples are outliers or not Samples with a score lower than anom_threshold_ are considered to be outliers. Parameters: Name Type Description Default X ~ArrayLike hape (n_samples, n_features). Input data required Returns: Type Description ndarray np.ndarray: shape (n_samples,) 1 for inlineres, -1 for outliers Source code in anlearn/loda.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def predict ( self , X : ArrayLike ) -> np . ndarray : \"\"\"Predict if samples are outliers or not Samples with a score lower than ``anom_threshold_`` are considered to be outliers. Args: X (ArrayLike): hape (n_samples, n_features). Input data Returns: np.ndarray: shape (n_samples,) 1 for inlineres, -1 for outliers \"\"\" check_is_fitted ( self , attributes = [ \"anom_threshold_\" ]) scores = self . score_samples ( X ) return np . where ( scores < self . anom_threshold_ , - 1 , 1 )","title":"predict()"},{"location":"reference/loda/#anlearn.loda.LODA.score_samples","text":"Anomaly scores for samples Average of the logarithm probabilities estimated of individual projections. Output is proportional to the negative log-likelihood of the sample, that means the less likely a sample is, the higher the anomaly value it receives[1]_. This score is reversed for Scikit learn compatibility. Parameters: Name Type Description Default X ~ArrayLike shape (n_samples, n_features). Input data required Returns: Type Description ndarray np.ndarray: shape (n_samples,) The anomaly score of the input samples. The lower, the more abnormal. Source code in anlearn/loda.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def score_samples ( self , X : ArrayLike ) -> np . ndarray : \"\"\"Anomaly scores for samples Average of the logarithm probabilities estimated of individual projections. Output is proportional to the negative log-likelihood of the sample, that means the less likely a sample is, the higher the anomaly value it receives[1]_. This score is reversed for Scikit learn compatibility. Args: X (ArrayLike): shape (n_samples, n_features). Input data Returns: np.ndarray: shape (n_samples,) The anomaly score of the input samples. The lower, the more abnormal. \"\"\" check_is_fitted ( self , attributes = [ \"projections_\" , \"hists_\" ]) w_X = X @ self . projections_ . T X_prob = [ hist . predict_proba ( w_x ) for hist , w_x in zip ( self . hists_ , w_X . T )] X_scores = np . mean ( np . log ( X_prob ), axis = 0 ) return X_scores","title":"score_samples()"},{"location":"reference/stats/","text":"IQR Interquartile range __init__ ( self , k = 1.5 , lower_quantile = 0.25 , upper_quantile = 0.75 , ensure_2d = True ) special Interquartile range Outlier deteciton method using Tukey's fences. If lower quantile is 0.25 ( Q_1 Q_1 lower quartile) and upper quantile is 0.75 ( Q_3 Q_3 upper quartile), then outlier is any observation outside the range: [Q_1 - k(Q_3 - Q_1); Q_3 + k(Q_3 - Q_1)] [Q_1 - k(Q_3 - Q_1); Q_3 + k(Q_3 - Q_1)] John Tukey proposed k=1.5 k=1.5 is an outlier, and k=3 k=3 is far out. Parameters: Name Type Description Default k float Outlier threshold. Defaults to 1.5. 1.5 lower_quantile float Lower quantile, from (0; 1). Defaults to 0.25. 0.25 upper_quantile float Upper quantile, from (0; 1). Defaults to 0.75. 0.75 ensure_2d bool Frobid input 1D arrays. Defaults to True. True Attrubutes lqv_ (float) : Lower quantile value estimated from the input data uqv_ (float) : Upper quantile value estimated from the input data iqr_ (float) : Interquartile range, uqv_ - lgv_ Exceptions: Type Description ValueError IQR: Lower quantile must be lower than upper quantile. Source code in anlearn/stats.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def __init__ ( self , k : float = 1.5 , lower_quantile : float = 0.25 , upper_quantile : float = 0.75 , ensure_2d : bool = True , ) -> None : \"\"\"Interquartile range Outlier deteciton method using Tukey's fences. If lower quantile is 0.25 ($Q_1$ lower quartile) and upper quantile is 0.75 ($Q_3$ upper quartile), then outlier is any observation outside the range: * $[Q_1 - k(Q_3 - Q_1); Q_3 + k(Q_3 - Q_1)]$ John Tukey proposed $k=1.5$ is an outlier, and $k=3$ is far out. Args: k (float, optional): Outlier threshold. Defaults to 1.5. lower_quantile (float, optional): Lower quantile, from (0; 1). Defaults to 0.25. upper_quantile (float, optional): Upper quantile, from (0; 1). Defaults to 0.75. ensure_2d (bool, optional): Frobid input 1D arrays. Defaults to True. Attrubutes: * lqv_ (float) : Lower quantile value estimated from the input data * uqv_ (float) : Upper quantile value estimated from the input data * iqr_ (float) : Interquartile range, `uqv_ - `lgv_ Raises: ValueError: IQR: Lower quantile must be lower than upper quantile. \"\"\" self . k = k self . ensure_2d = ensure_2d self . lower_quantile = lower_quantile self . upper_quantile = upper_quantile if lower_quantile >= upper_quantile : raise ValueError ( \"IQR: Lower quantile must be lower than upper quantile.\" ) fit ( self , X , y = None ) Fit estimator Parameters: Name Type Description Default X ~ArrayLike shape (n_samples, 1) or (n_samples,) if ensure_2d is False required y Optional[~ArrayLike] ignored. Present for API consistency by convention. None Returns: Type Description IQR IQR: Fitted estimator Source code in anlearn/stats.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def fit ( self , X : ArrayLike , y : Optional [ ArrayLike ] = None ) -> \"IQR\" : \"\"\"Fit estimator Args: X (ArrayLike, optional): shape (n_samples, 1) or (n_samples,) if `ensure_2d` is False y (ArrayLike, optional) : ignored. Present for API consistency by convention. Returns: IQR: Fitted estimator \"\"\" raw_data = check_array ( X , force_all_finite = True , ensure_2d = self . ensure_2d ) . flatten () self . lqv_ , self . uqv_ = np . quantile ( raw_data , ( 0.25 , 0.75 )) self . iqr_ = self . uqv_ - self . lqv_ return self predict ( self , X ) Predict if samples are outliers or not Samples with a score lower than k are considered to be outliers. Parameters: Name Type Description Default X ~ArrayLike shape (n_samples, n_features). Input data required Returns: Type Description ndarray np.ndarray: shape (n_samples,) 1 for inlineres, -1 for outliers Source code in anlearn/stats.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def predict ( self , X : ArrayLike ) -> np . ndarray : \"\"\"Predict if samples are outliers or not Samples with a score lower than `k` are considered to be outliers. Args: X (ArrayLike): shape (n_samples, n_features). Input data Returns: np.ndarray: shape (n_samples,) 1 for inlineres, -1 for outliers \"\"\" scores = self . score_samples ( X ) return np . where ( scores < - self . k , - 1 , 1 ) score_samples ( self , X ) Score samples Score is comuputed as distance from interval [Q_{lower}; Q_{upper}] [Q_{lower}; Q_{upper}] divided by interquartile range. score = distance(data, (lqv, uqv)) / iqr score = distance(data, (lqv, uqv)) / iqr . Score is inverted for scikit-learn compatibility Parameters: Name Type Description Default X ~ArrayLike shape (n_samples, 1) or (n_samples,) if ensure_2d is False Input data required Returns: Type Description ndarray np.ndarray: shape (n_samples,) The outlier score of the input samples. The lower, the more abnormal. Source code in anlearn/stats.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def score_samples ( self , X : ArrayLike ) -> np . ndarray : \"\"\"Score samples Score is comuputed as distance from interval $[Q_{lower}; Q_{upper}]$ divided by interquartile range. $score = distance(data, (lqv, uqv)) / iqr$. Score is inverted for scikit-learn compatibility Args: X (ArrayLike): shape (n_samples, 1) or (n_samples,) if `ensure_2d` is False Input data Returns: np.ndarray: shape (n_samples,) The outlier score of the input samples. The lower, the more abnormal. \"\"\" check_is_fitted ( self , attributes = [ \"lqv_\" , \"uqv_\" , \"iqr_\" ]) raw_data = check_array ( X , force_all_finite = True , ensure_2d = self . ensure_2d ) . flatten () scores = np . zeros ( shape = raw_data . shape [ 0 ]) l_lqv = raw_data < self . lqv_ scores [ l_lqv ] = ( raw_data [ l_lqv ] - self . lqv_ ) / self . iqr_ g_uqv = raw_data > self . uqv_ scores [ g_uqv ] = ( raw_data [ g_uqv ] - self . uqv_ ) / self . iqr_ return - np . abs ( scores )","title":"anlearn.stats"},{"location":"reference/stats/#anlearn.stats","text":"","title":"anlearn.stats"},{"location":"reference/stats/#anlearn.stats.IQR","text":"Interquartile range","title":"IQR"},{"location":"reference/stats/#anlearn.stats.IQR.__init__","text":"Interquartile range Outlier deteciton method using Tukey's fences. If lower quantile is 0.25 ( Q_1 Q_1 lower quartile) and upper quantile is 0.75 ( Q_3 Q_3 upper quartile), then outlier is any observation outside the range: [Q_1 - k(Q_3 - Q_1); Q_3 + k(Q_3 - Q_1)] [Q_1 - k(Q_3 - Q_1); Q_3 + k(Q_3 - Q_1)] John Tukey proposed k=1.5 k=1.5 is an outlier, and k=3 k=3 is far out. Parameters: Name Type Description Default k float Outlier threshold. Defaults to 1.5. 1.5 lower_quantile float Lower quantile, from (0; 1). Defaults to 0.25. 0.25 upper_quantile float Upper quantile, from (0; 1). Defaults to 0.75. 0.75 ensure_2d bool Frobid input 1D arrays. Defaults to True. True Attrubutes lqv_ (float) : Lower quantile value estimated from the input data uqv_ (float) : Upper quantile value estimated from the input data iqr_ (float) : Interquartile range, uqv_ - lgv_ Exceptions: Type Description ValueError IQR: Lower quantile must be lower than upper quantile. Source code in anlearn/stats.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def __init__ ( self , k : float = 1.5 , lower_quantile : float = 0.25 , upper_quantile : float = 0.75 , ensure_2d : bool = True , ) -> None : \"\"\"Interquartile range Outlier deteciton method using Tukey's fences. If lower quantile is 0.25 ($Q_1$ lower quartile) and upper quantile is 0.75 ($Q_3$ upper quartile), then outlier is any observation outside the range: * $[Q_1 - k(Q_3 - Q_1); Q_3 + k(Q_3 - Q_1)]$ John Tukey proposed $k=1.5$ is an outlier, and $k=3$ is far out. Args: k (float, optional): Outlier threshold. Defaults to 1.5. lower_quantile (float, optional): Lower quantile, from (0; 1). Defaults to 0.25. upper_quantile (float, optional): Upper quantile, from (0; 1). Defaults to 0.75. ensure_2d (bool, optional): Frobid input 1D arrays. Defaults to True. Attrubutes: * lqv_ (float) : Lower quantile value estimated from the input data * uqv_ (float) : Upper quantile value estimated from the input data * iqr_ (float) : Interquartile range, `uqv_ - `lgv_ Raises: ValueError: IQR: Lower quantile must be lower than upper quantile. \"\"\" self . k = k self . ensure_2d = ensure_2d self . lower_quantile = lower_quantile self . upper_quantile = upper_quantile if lower_quantile >= upper_quantile : raise ValueError ( \"IQR: Lower quantile must be lower than upper quantile.\" )","title":"__init__()"},{"location":"reference/stats/#anlearn.stats.IQR.fit","text":"Fit estimator Parameters: Name Type Description Default X ~ArrayLike shape (n_samples, 1) or (n_samples,) if ensure_2d is False required y Optional[~ArrayLike] ignored. Present for API consistency by convention. None Returns: Type Description IQR IQR: Fitted estimator Source code in anlearn/stats.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def fit ( self , X : ArrayLike , y : Optional [ ArrayLike ] = None ) -> \"IQR\" : \"\"\"Fit estimator Args: X (ArrayLike, optional): shape (n_samples, 1) or (n_samples,) if `ensure_2d` is False y (ArrayLike, optional) : ignored. Present for API consistency by convention. Returns: IQR: Fitted estimator \"\"\" raw_data = check_array ( X , force_all_finite = True , ensure_2d = self . ensure_2d ) . flatten () self . lqv_ , self . uqv_ = np . quantile ( raw_data , ( 0.25 , 0.75 )) self . iqr_ = self . uqv_ - self . lqv_ return self","title":"fit()"},{"location":"reference/stats/#anlearn.stats.IQR.predict","text":"Predict if samples are outliers or not Samples with a score lower than k are considered to be outliers. Parameters: Name Type Description Default X ~ArrayLike shape (n_samples, n_features). Input data required Returns: Type Description ndarray np.ndarray: shape (n_samples,) 1 for inlineres, -1 for outliers Source code in anlearn/stats.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def predict ( self , X : ArrayLike ) -> np . ndarray : \"\"\"Predict if samples are outliers or not Samples with a score lower than `k` are considered to be outliers. Args: X (ArrayLike): shape (n_samples, n_features). Input data Returns: np.ndarray: shape (n_samples,) 1 for inlineres, -1 for outliers \"\"\" scores = self . score_samples ( X ) return np . where ( scores < - self . k , - 1 , 1 )","title":"predict()"},{"location":"reference/stats/#anlearn.stats.IQR.score_samples","text":"Score samples Score is comuputed as distance from interval [Q_{lower}; Q_{upper}] [Q_{lower}; Q_{upper}] divided by interquartile range. score = distance(data, (lqv, uqv)) / iqr score = distance(data, (lqv, uqv)) / iqr . Score is inverted for scikit-learn compatibility Parameters: Name Type Description Default X ~ArrayLike shape (n_samples, 1) or (n_samples,) if ensure_2d is False Input data required Returns: Type Description ndarray np.ndarray: shape (n_samples,) The outlier score of the input samples. The lower, the more abnormal. Source code in anlearn/stats.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def score_samples ( self , X : ArrayLike ) -> np . ndarray : \"\"\"Score samples Score is comuputed as distance from interval $[Q_{lower}; Q_{upper}]$ divided by interquartile range. $score = distance(data, (lqv, uqv)) / iqr$. Score is inverted for scikit-learn compatibility Args: X (ArrayLike): shape (n_samples, 1) or (n_samples,) if `ensure_2d` is False Input data Returns: np.ndarray: shape (n_samples,) The outlier score of the input samples. The lower, the more abnormal. \"\"\" check_is_fitted ( self , attributes = [ \"lqv_\" , \"uqv_\" , \"iqr_\" ]) raw_data = check_array ( X , force_all_finite = True , ensure_2d = self . ensure_2d ) . flatten () scores = np . zeros ( shape = raw_data . shape [ 0 ]) l_lqv = raw_data < self . lqv_ scores [ l_lqv ] = ( raw_data [ l_lqv ] - self . lqv_ ) / self . iqr_ g_uqv = raw_data > self . uqv_ scores [ g_uqv ] = ( raw_data [ g_uqv ] - self . uqv_ ) / self . iqr_ return - np . abs ( scores )","title":"score_samples()"}]}